# Batch Prediction with Long Context and Context Caching ğŸš€ğŸ§ 

A Streamlit application that demonstrates efficient batch prediction using Google's Gemini API, featuring long context handling, caching, and interactive conversation history.

## Features

- ğŸ”„ **Batch Processing**: Process multiple questions efficiently in batches
- ğŸ“ **Long Context Handling**: Handle large video transcripts effectively
- ğŸ’¾ **Context Caching**: Cache responses to avoid redundant API calls
- ğŸ”— **Interconnected Questions**: Generate and track related questions
- ğŸ’¬ **Conversation History**: Keep track of all Q&A interactions
- ğŸ“œ **Transcript Display**: View and reference video transcripts easily
- ğŸ¯ **Smart Question Suggestions**: Get AI-generated relevant questions
- âš¡ **Interactive UI**: Clean, responsive interface with two-column layout

## Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/your-repo.git
cd your-repo
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up your environment variables:
   - Create a `.env` file in the project root
   - Add your Google API key:
     ```
     google_api_key=your_api_key_here
     ```

## Usage

1. Run the Streamlit app:
```bash
streamlit run app.py
```

2. Enter a YouTube URL in the input field
   - The app supports any public YouTube video with available transcripts
   - Transcripts are automatically extracted and processed

3. Interact with the content:
   - View the video transcript in the expandable section
   - Ask your own questions (multiple questions supported)
   - Click on suggested questions for quick insights
   - Track conversation history in the right panel

## Project Structure

```
project/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ requirements.txt       # Project dependencies
â”œâ”€â”€ .env                  # Environment variables
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ batch_processor.py    # Handles batch processing of questions
    â”œâ”€â”€ context_cache.py      # Manages response caching
    â”œâ”€â”€ youtube_handler.py    # Handles YouTube transcript extraction
    â””â”€â”€ text_chunker.py       # Manages text chunking for long contexts
```

## Technical Details

### Context Management
- Efficient handling of long video transcripts
- Smart chunking for optimal context processing
- Caching system to store and retrieve responses

### Caching System
- TTL-based cache for responses
- Memory-efficient storage
- Automatic cache invalidation

### Batch Processing
- Parallel question processing
- Optimized API calls
- Response aggregation

### UI Components
- Two-column layout for better organization
- Interactive question suggestions
- Persistent conversation history
- Expandable transcript viewer

## Error Handling

The application includes robust error handling for:
- Invalid YouTube URLs
- Missing transcripts
- API failures
- Rate limiting
- Memory issues

## Troubleshooting

### Common Issues and Solutions

1. **API Key Issues**
   - Verify your API key is correctly set in the `.env` file
   - Check if the API key has the necessary permissions

2. **YouTube Transcript Issues**
   - Ensure the video has available captions
   - Check if the video is public and accessible

3. **Model Initialization Issues**
   - Verify internet connectivity
   - Check if the Gemini API service is available

4. **Memory Issues**
   - Clear the cache if processing large transcripts
   - Reduce batch size for very long videos

### SSL Certificate Issues
If you encounter SSL certificate errors:

1. Install/upgrade certifi:
```bash
pip install --upgrade certifi
```

2. Update your certificates:
```bash
python -m pip install --upgrade pip
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request. 
